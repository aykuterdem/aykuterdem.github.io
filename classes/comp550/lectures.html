<!DOCTYPE HTML>
<!--
	Solarize by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>COMP550: Advances in Deep Learning</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script  src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<script defer src="https://use.fontawesome.com/releases/v5.0.13/js/all.js"></script>
		<link href="css/fontawesome.css" rel="stylesheet">
		<link href="css/brands.css" rel="stylesheet">
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
	</head>
	<body class="homepage">

		<!-- Header Wrapper -->
			<div class="wrapper style11">
			
			<!-- Header -->
				<div id="header" style="color: #bbb">
					<div class="container">
						<!-- Logo -->
							<h1><a href="#" id="logo">COMP547</a></h1>

						<nav id="nav">
								<ul>
									<li class="active"><a href="index.html#div_courseinfo">About</a></li>
									<li><a href="index.html#div_schedule">Schedule</a></li>
									<li><a href="presentations.html">Presentations</a></li>
									<li><a href="project.html">Project</a></li>
									<li>
										<a href="http://ku.blackboard.com"><i class="fas fa-chalkboard"></i></a> &middot;
										<a href="https://join.slack.com/t/comp550advances-in-dl/signup"><i class="fab fa-slack fa-lg"></i></a>
									</li>
								</ul>
							</nav>
	
					</div>
				</div>
				
			<!-- Banner -->
				<div id="banner" style="color: black">
					<section class="container">
						<h2>COMP550: Advances in Deep Learning</h2>
						<span>Fall 2021</span>
					</section>
				</div>
			</div>

			<!-- Course Information -->
			<div class="wrapper style2">
			<section class="container">
				<h1 class="content-subhead">Detailed Syllabus and Lectures</h1>

				<!-- Topic 11 -->
   				<hr>
  				<h2>Topic 11: Deep Implicit Layers <!-- (<a href="">paper 1 slides</a>) (<a href="">paper 1 video</a>)   &middot; (<a href="">paper 2 slides</a>) (<a href="">paper 2 video)--></h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://proceedings.neurips.cc/paper/2020/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf">Multiscale Deep Equilibrium Models</a>. Shaojie Bai, Vladlen Koltun, J. Zico Kolter. NeurIPS 2020.</li>
  					<li><a href="https://arxiv.org/abs/2010.08188">Vid-ODE: Continuous-Time Video Generation with Neural Ordinary Differential Equation</a>. Sunghyun Park, Kangyeol Kim, Junsoo Lee, Jaegul Choo, Joonseok Lee, Sookyung Kim, Edward Choi. AAAI 2021.</li>
  				</ul>
  				<p>
	  				
	  			<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li>Zico Kolter, David Duvenaud, Matt Johnson's tutorial on <a href="http://implicit-layers-tutorial.org">Deep Implicit Layers - Neural ODEs, Deep Equilibirum Models, and Beyond</a>, NeurIPS 2020.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 10 -->
   				<hr>
  				<h2>Topic 10: Vision Transformers (<a href="https://docs.google.com/presentation/d/1yThC7ekCPRTZ63WLUuK_ksPxgf6NlBUb2TYQAR1JISk/edit#slide=id.p">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=a08bafe0-0aaa-475a-ba4f-ae0400a67096">paper 1 video</a>)  <!-- &middot; (<a href="">paper 2 slides</a>) (<a href="">paper 2 video)--></h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://openreview.net/forum?id=YicbFdNTTy">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. ICLR 2021.</li>
  					<li><a href="https://arxiv.org/abs/2103.13413">Vision Transformers for Dense Prediction</a>. Ren Ranftl, Alexey Bochkovskiy, Vladlen Koltun. ICCV 2021.</li>
  				</ul>
  				<p>
	  				
	  			<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://arxiv.org/abs/2101.01169">Transformers in Vision: A Survey</a>. Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, Mubarak Shah. ACM Computing Surveys, 2021.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 9 -->
   				<hr>
  				<h2>Topic 9: Memory and Attention (<a href="https://docs.google.com/presentation/d/1jZZVIGr29dYneo_Mx3FTDOFaBNFKyGJN2S120mpxJt4/edit#slide=id.p">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b9070d25-8783-4ae3-83ad-adfd00a2b31b">paper 1 video</a>)  &middot; (<a href="https://docs.google.com/presentation/d/1bgy2XSmHrN-uZ1DpRadwVCtK5VtqFFqKOC-43EtSsY4/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e588ae82-cc0a-4fa5-850b-adff00a3dce1">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Kim_Learning_to_Simulate_Dynamic_Environments_With_GameGAN_CVPR_2020_paper.html">Learning to Simulate Dynamic Environments With GameGAN</a>. Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler. CVPR 2020.</li>
  					<li><a href="https://openreview.net/forum?id=QoWatN-b8T">Kanerva++: Extending the Kanerva Machine With Differentiable, Locally Block Allocated Latent Memory</a>. Jason Ramapuram, Yan Wu, Alexandros Kalousis. ICLR 2021.
</li>
  				</ul>
  				<p>
	  				
	  			<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://baicsworkshop.github.io/pdf/BAICS_22.pdf">On Memory in Human and Artificial Language Processing Systems</a>. Aida Nematzadeh, Sebastian Ruder & Dani Yogatama, Workshop on Bridging AI and Cognitive Science, ICLR 2020</li>
	  				<li>Alex Graves' lecture on <a href="https://www.youtube.com/watch?v=AIiwuClvH6k">Attention and Memory in Deep Learning</a>, DeepMind x UCL Deep Learning Lectures, 2020.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 8 -->
   				<hr>
  				<h2>Topic 8: Neural rendering (<a href="https://docs.google.com/presentation/d/1stK4s-AvOQWIDIqkVJ-gJ7CGa7ASe2k_4ExI4zbK3Es/edit#slide=id.p">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=3107945e-5fdc-4b62-a0ea-adf600a2de70">paper 1 video</a>)  &middot; (<a href="https://docs.google.com/presentation/d/18w8vhUKkX5hCLSnPw8pxuSaymAOJ18n8GdwXkZUdzhc/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e2a3d3ee-e4ad-422b-965a-adf800a1e33b">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://arxiv.org/abs/2106.11303">Understanding Object Dynamics for Interactive Image-to-Video Synthesis</a>. Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Björn Ommer. CVPR 2020.</li>
  					<li><a href="https://arxiv.org/abs/2108.05892">PixelSynth: Generating a 3D-Consistent Experience from a Single Image</a>. Chris Rockwell, David F. Fouhey, Justin Johnson. ICCV 2021.
</li>
  				</ul>
  				<p>
	  				
	  			<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li>Advances in Neural Rendering (<a href="https://www.youtube.com/watch?v=otly9jcZ0Jg">Part 1</a>) (<a href="https://www.youtube.com/watch?v=aboFl5ozImM">Part 2</a>). Michael Zollhöfer et al., SIGGRAPH 2021 Courses.</li>
  				</ul>
  				</p>
	  				
	  			<!-- Topic 7 -->
   				<hr>
  				<h2>Topic 7: Dynamic networks (<a href="https://docs.google.com/presentation/d/11ks57Ynsj4VilZd8j0OP03xULiElzIxvCzV8p-uDWO8/edit#slide=id.p">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=7afe8f3b-4e53-4018-a4ef-adef00a4584e">paper 1 video</a>) &middot; (<a href="https://docs.google.com/presentation/d/1Vc4yRufgmojCCjl4aXCLKEO1bk6EIGUrVDYVhkqAgyM/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=4e816d66-bd79-4775-96ac-adf100af12a4">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://openreview.net/pdf?id=ByxT7TNFvH">Semantically-Guided Representation Learning for Self-Supervised Monocular Depth</a>. Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, Adrien Gaidon. ICLR 2020.</li>
  					<li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shaham_Spatially-Adaptive_Pixelwise_Networks_for_Fast_Image_Translation_CVPR_2021_paper.pdf">Spatially-Adaptive Pixelwise Networks for Fast Image Translation</a>. Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer Michaeli. CVPR 2021.
</li>
  				</ul>
  				<p>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://arxiv.org/pdf/2102.04906.pdf">Dynamic Neural Networks: A Survey</a>. Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021, appeared online.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 6 -->
   				<hr>
  				<h2>Topic 6: Neural implicit representations  (<a href="https://docs.google.com/presentation/d/1p4NoCY56_XR2NLjwyVGVLVqGmCHD_rDmD9zB2S88YoM/edit#slide=id.p">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=8b18af57-a7ee-473c-accd-adda00b1cb4b">paper 1 video</a>) &middot; (<a href="https://docs.google.com/presentation/d/1Nxzakmpz50ZUNyxzVXz1vAcWlm3rQmRf0G8dINvSQlI/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=8544658e-d21e-4722-b856-addc0085e0bf">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://proceedings.neurips.cc/paper/2020/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf">Implicit neural representations with periodic activation functions</a>. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein. NeurIPS 2020.</li>
  					<li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chan_Pi-GAN_Periodic_Implicit_Generative_Adversarial_Networks_for_3D-Aware_Image_Synthesis_CVPR_2021_paper.pdf">pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis</a>. Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein. CVPR 2021.</li>
  				</ul>
  				<p>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://github.com/vsitzmann/awesome-implicit-representations">Awesome Implicit Neural Representations</a>, A curated list of resources on implicit neural representations, Vincent Sitzmann.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 5 -->
   				<hr>
  				<h2>Topic 5: Neuro-symbolic approaches (<a href="https://docs.google.com/presentation/d/1DybVMaXkAnluvsN_Jlaa3g6euMsfwHm5_jLOXRBQzRw/edit?usp=sharing">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=55f3b223-ff90-42a3-adaf-add300a8a629">paper 1 video</a>) &middot; (<a href="https://docs.google.com/presentation/d/1OjT2uQM31jfacGXmpVkW3ETMwHGmjTFziHb3U7yZYhU/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1bffce4e-a2ec-4c90-ba1d-add500a3b203">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://openreview.net/forum?id=bhCDO_cEGCz">Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning</a>. Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B. Tenenbaum, Chuang Gan. ICLR 2021.</li>
  					<li><a href="https://aclanthology.org/2021.acl-long.159.pdf">PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World</a>. Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, Yejin Choi. ACL 2021.</li>
  				</ul>
  				<p>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://www.youtube.com/watch?v=4PuuziOgSU4">Neurosymbolic AI</a>. David Cox, MIT 6.S191, 2020.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 4 -->
   				<hr>
  				<h2>Topic 4: Object-centric representation learning (<a href="https://docs.google.com/presentation/d/1V9_2WR4IjXoytG7XcEseeHQ5XSwCgYgLuHzc8e-iDvA/edit">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=27abf0d4-e46d-49a3-a1f1-adcc00a7e15d">paper 1 video</a>)  &middot; (<a href="https://docs.google.com/presentation/d/1bQ2mfe7D6RcS4sShSYXVbyPI3sM_bmZUmNKRjl3opqE/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1cdb816e-7faf-4032-9ef8-adce00a3ec10">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://proceedings.neurips.cc/paper/2020/hash/4324e8d0d37b110ee1a4f1633ac52df5-Abstract.html">Learning Physical Graph Representations from Visual Scenes</a>. Daniel Bear, Chaofei Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz, Li F. Fei-Fei, Jiajun Wu, Josh Tenenbaum, Daniel L. Yamins. NeurIPS 2020.</li>
  					<li><a href="https://arxiv.org/abs/2106.03849">SIMONe: View-Invariant, Temporally-Abstracted Object Representations via Unsupervised Video Decomposition</a>. Rishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matthew Botvinick, Alexander Lerchner, Christopher P. Burgess. arXiv preprint arXiv:2106.03849, 2021.</li>
  				</ul>
  				<p>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://slideslive.com/38930701/what-are-objects?locale=cs">What are Objects?</a>. Klaus Greff, ICML 2020 Workshop on Object-Oriented Learning: Perception, Representation, and Reasoning, 2020.</li>
	  				<li><a href="https://slideslive.com/38938235/what-is-an-object?ref=speaker-12640-latest">What is an object?</a>. Irina Higgins, NeurIPS 2020 Workshop on Object Representations for Learning and Reasoning, 2020.</li>
  				</ul>
  				</p>
  				
				<!-- Topic 3 -->
   				<hr>
  				<h2>Topic 3: Graph neural networks (<a href="https://docs.google.com/presentation/d/1hCPwmSfiHOhWHNRsXyVVtz5nsd9wbTHu9fJSRPkEaD8/edit#slide=id.p">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=7dbbbf54-8204-48da-8de5-adc500a37a25">paper 1 video</a>) &middot; (<a href="https://docs.google.com/presentation/d/1FJHenfS-NYrweLGy_4qj-9DbOQZShqXDSTXE--4e_5c/edit?usp=sharing">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=2b945719-16bd-4e6d-82ac-adc700a39bec">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://openreview.net/forum?id=rJgbSn09Ym">Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids</a>. Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, Antonio Torralba. ICLR 2019.</li>
  					<li><a href="https://proceedings.neurips.cc/paper/2020/hash/6822951732be44edf818dc5a97d32ca6-Abstract.html">Causal Discovery in Physical Systems from Videos</a>. Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, Animesh Garg. NeurIPS 2020.</li>
  				</ul>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://www.youtube.com/watch?v=sTGKOUzIpaQ">Principles and applications of relational inductive biases in deep learning</a>. Kelsey Allen, MIT CBMM Talks, 2019.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 2 -->
   				<hr>
  				<h2>Topic 2: Multimodal representation learning (<a href="https://docs.google.com/presentation/d/1gFjNeVjplaenebku1bMtgLJ_sDmLOVlJpSTJ2fOroHA/edit?usp=sharing">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e360ffaf-f3e5-428d-8211-adbe00a589ac">paper 1 video</a>) &middot; (<a href="https://docs.google.com/presentation/d/1_S44ikDgR0UObHFZTf-PsORrZag7lqENtk7XkHwGM6Y/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=03023168-734a-47e5-93f4-adc000b3c58b">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://arxiv.org/abs/1911.09602">Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech</a>. David Harwath, Wei-Ning Hsu, James Glass. ICLR 2020.</li>
  					<li><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740426.pdf">Learning to Learn Words from Visual Scenes</a>. Dídac Surís, Dave Epstein, Heng Ji, Shih-Fu Chang, Carl Vondrick. ECCV 2020.</li>
  				</ul>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://arxiv.org/abs/1912.05877">Extending Machine Language Models toward Human-Level Language Understanding</a>. James L. McClelland, Felix Hill, Maja Rudolph, Jason Baldridge, Hinrich Schütze. arXiv preprint arXiv:1912.05877, 2020.</li>
  				</ul>
  				</p>
  				
  				<!-- Topic 1 -->
   				<hr>
  				<h2>Topic 1: Compositionality and systematic generalization (<a href="https://docs.google.com/presentation/d/16jrJj6evOTBSS5H5nBEmfCAVddL86Olrv2zxHjqdQHY/edit#slide=id.p">paper 1 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=97d79759-0d31-4db0-bddd-adb700a64c30">paper 1 video</a>) &middot; (<a href="https://docs.google.com/presentation/d/1Rx9m5Jk8lrakZycnAFGLXzHW6yoQdmKmDgizVvzQzPk/edit#slide=id.p">paper 2 slides</a>) (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=f2ef2810-6594-492b-8ac3-adb900b5b82d">paper 2 video)</h2>
  				<p><i></i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Readings:</h3>
  				<ul class="default">
  					<li><a href="https://proceedings.neurips.cc/paper/2020/hash/e5a90182cc81e12ab5e72d66e0b46fe3-Abstract.html">A Benchmark for Systematic Generalization in Grounded Language Understanding</a>. Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, Brenden M. Lake. NeurIPS 2020.</li>
  					<li><a href="https://openreview.net/forum?id=bKBhQhPeKaF">Benchmark for Compositional Text-to-Image Synthesis</a>. Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, Anna Rohrbach. NeurIPS 2021 Datasets and Benchmarks Track.</li>
  				</ul>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://jair.org/index.php/jair/article/view/11674/26576">Compositionality decomposed: how do neural networks generalise?</a>. Hupkes D., Dankers V., Mul M. and Bruni E. JAIR, Vol 67, 220.</li>
  					<li><a href="https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf">Connectionism and cognitive architecture: A critical analysis</a>. Jerry A Fodor and Zenon W Pylyshyn. Cognition, 28(1):3–71, 1988.</li>
  				</ul>
  				</p>
  				


<!-- Lecture 2 -->  						
  				<hr>
  				<h2>Lecture 2: A Look at Deep Learning Landscape (<a href="slides/lec2-introduction.pdf">slides</a>)</h2>
  				<p><i>course information, unsupervised learning</i></p>
  				
  				<p>Please study the following material in preparation for the class:</p>
  				<p>
  				<h3>Required Reading:</h3>
	  			<ul class="default">
		  			<li><a href="https://arxiv.org/abs/1604.00289">Building Machines That Learn and Think Like People</a>. Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman. Behavioral and Brain Sciences, Vol. 40, 2016</li>
	  				<li><a href="https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext">Deep Learning for AI</a>. Yoshua Bengio, Yann Lecun, Geoffrey Hinton. Communications of the ACM, July 2021.</li>	
	  				<li><a href="https://arxiv.org/abs/2104.12871">Why AI is Harder Than We Think</a>. Melanie Mitchell. arXiv preprint, 2021.</li>
	  				<li><a href="https://people.csail.mit.edu/brooks/papers/AIM-1293.pdf">Intelligence without Reason</a>. Rodney Brooks. 1991.</li>
	  			</ul><br>
  				</p>
	  			
	  			<h3>Suggested Video Material:</h3>
	  			<ul class="default">
		  			<li>Eric Horvitz and Peter Norvig's discussion on <a href="https://www.youtube.com/watch?v=rtmQ3xlt-4A">The Challenge & Promise of Artificial Intelligence</a>, 2012.</li>
	  			</ul>
  				</p>
  				
  				<h2>Lecture 1: Introduction to the course (<a href="https://kocuni.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=524678db-2c44-47a9-9c8c-adb00078609a">video</a>)</h2>
  				<p><i>course information, coarse format, presentation roles</i></p>
  				<p>
  				<hr>
  				</div>
  

	<!-- Footer -->
		<div id="footer">
			<!-- Copyright -->
				<div id="copyright">
					design: <a href="http://templated.co">templated.co</a>
				</div>			
		</div>

	</body>
</html>
