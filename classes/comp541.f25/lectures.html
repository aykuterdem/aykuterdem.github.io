<!DOCTYPE HTML>
<!--
	Solarize by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>COMP541: Deep Learning</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script  src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<script defer src="https://use.fontawesome.com/releases/v5.0.13/js/all.js"></script>
		<link href="css/fontawesome.css" rel="stylesheet">
		<link href="css/brands.css" rel="stylesheet">
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
	</head>
	<body class="homepage">

		<!-- Header Wrapper -->
			<div class="wrapper style11">
			
			<!-- Header -->
				<div id="header" style="color: #bbb">
					<div class="container">
						<!-- Logo -->
							<h1><a href="#" id="logo">COMP541</a></h1>

						<nav id="nav">
								<ul>
									<li class="active"><a href="index.html#div_courseinfo">About</a></li>
									<li><a href="index.html#div_schedule">Schedule</a></li>
									<li><a href="project.html">Project</a></li>
									<li><a href="assignments.html">Assignments</a></li>
									<li><a href="tutorials.html">Tutorials</a></li>
									<li><a href="presentations.html">Presentations</a></li>
									<li>
										<a href="https://learn.hub.ku.edu.tr/my/courses.php"><i class="fas fa-chalkboard"></i></a><!-- &middot;
										<a href="https://edstem.org/us/join/SdAA2p"<b>ed</b></a>-->
									</li>								
								</ul>
							</nav>
	
					</div>
				</div>
				
			<!-- Banner -->
				<div id="banner" style="color: black">
					<section class="container">
						<h2>COMP541: Deep Learning</h2>
						<span>Fall 2025</span>
					</section>
				</div>
			</div>

			<!-- Course Information -->
			<div class="wrapper style2">
			<section class="container">
            <h1 class="content-subhead">Detailed Syllabus and Lectures</h1>
<!-- Lecture 14 -->
            <!--  
              <li>[Blog post] <a href="https://sebastianraschka.com/blog/2025/understanding-multimodal-llms.html">Understanding Multimodal LLMs</a>, Sebastian Raschka.</li>
              -->

<!-- Lecture 13 -->
<!--<hr>
<h2>Lecture 13: Multimodal Pretraining (<a href="slides/lec13-multimodal-pretraining.pdf">slides</a>)</h2>
<p><i>vision-language landscape before Transformers, vision-language pretraining, multimodal large language models</i></p>
<p>Please study the following material in preparation for the class:</p>
<h3>Required Reading:</h3>
<ul class="default">
    <li><a href="https://arxiv.org/pdf/2401.13601.pdf">MM-LLMs: Recent Advances in MultiModal Large Language Models</a>, Duzhen Zhang et al., 2025.</li>
</ul>
</p>
<h3>Suggested Video Material:</h3>
<ul class="default">
	<li>Douwe Kiela's Stanford CS224N lecture on <a href="https://www.youtube.com/watch?v=5vfIT5LOkR0">Multimodal Deep Learning</a></li>
</ul><br>
</p>
<h3>Additional Resources:</h3>
<ul class="default">
   <li>[Blog post] <a href="https://huyenchip.com/2023/10/10/multimodal.html">Multimodality and Large Multimodal Models</a>, Chip Huyen.</li>
   <li>[Blog post] <a href="https://sebastianraschka.com/blog/2025/understanding-multimodal-llms.html">Understanding Multimodal LLMs</a>, Sebastian Raschka.</li>
</ul>-->

<!-- Lecture 12 -->
<!--<hr>
<h2>Lecture 12: Adapting LLMs (<a href="slides/lec12-adapting-lllms.pdf">slides</a>)</h2>
<p><i>fine-tuning and fine-tuning methods, instruction tuning, learning from human feedback</i></p>
<p>Please study the following material in preparation for the class:</p>
<h3>Required Reading:</h3>
<ul class="default">
  <li><a href="https://openreview.net/forum?id=gEZrGCozdqR">Finetuned Language Models Are Zero-Shot Learners</a>, Jason Wei et al., ICLR 2022.</li>
  <li><a href="https://openai.com/index/instruction-following/">Aligning language models to follow instructions</a>, Ryan Lowe, Jan Leike, 2022.</li>
</ul>
<p>
<h3>Suggested Video Material:</h3>
<ul class="default">
  <li><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">Instruction finetuning and RLHF</a>, Hyung Won Chung</li>
</ul><br>
</p>
<p>
<h3>Additional Resources:</h3>
<ul class="default">
  <li><a href="https://openreview.net/forum?id=nZeVKeeFYf9">LoRA: Low-Rank Adaptation of Large Language Models</a>, Edward J Hu et al., ICLR 2022.</li>
  <li><a href="https://arxiv.org/abs/2110.04366">Towards a Unified View of Parameter-Efficient Transfer Learning</a>, Junxian He et al., ICLR 2022.</li>
  <li><a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a>, Victor Sanh et al., ICLR 2022.</li>
  <li><a href="https://openreview.net/pdf?id=OUIFPHEgJU">QLoRA: Efficient Finetuning of Quantized LLMs</a>, Tim Dettmers, NeurIPS 2023.</li>
  <li><a href="https://arxiv.org/abs/2106.10199">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</a>, Elad Ben Zaken et al., ACL 2022.</li>
  <li><a href="https://arxiv.org/pdf/2308.10792">Instruction Tuning for Large Language Models: A Survey</a>, Shengyu Zhang et al., December 2025.</li>
  <li><a hrewf="https://allenai.org/olmo">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>, Rafael Rafailov et al., NeurIPS 2023.</li>
</ul>
</p>-->

<!-- Lecture 11 -->
<!--<hr>
<h2>Lecture 11: Large Language Models (<a href="slides/lec11-large-language-models.pdf">slides</a>)</h2>
<p><i>GPT-3, understanding in-context learning, scaling laws, Llama 3, other LLMs, long context models</i></p>
<p>Please study the following material in preparation for the class:</p>
<h3>Required Reading:</h3>
<ul class="default">
<li><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>, Rich Sutton, March 2019.</li>
<li><a href="https://arxiv.org/abs/2402.06196">Large Language Models: A Survey</a>, Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, ArXiv Preprint, 2025.
</li>
</ul>
<p>
<h3>Suggested Video Material:</h3>
<ul class="default">
  <li><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">Intro to Large Language Models</a>, Andrej Karpathy</li>
</ul><br>
</p>
<p>
<h3>Additional Resources:</h3>
<ul class="default">
  <li>[Blog post] <a href="https://lena-voita.github.io/nlp_course/language_modeling.html">Language Modeling</a>, Lena Voita.</li>
  <li>[Blog post] <a href="https://ai.stanford.edu/blog/understanding-incontext/">How does in-context learning work? A framework for understanding the differences from traditional supervised learning</a>, Sang Michael Xie and Sewon Min</li>
  <li><a href="https://ai.meta.com/blog/meta-llama-3-1/">Introducing Llama 3.1: Our most capable models to date</a>, Meta AI</li>
  <li><a hrewf="https://allenai.org/olmo">OLMo Language Models</a>, Allen AI</li>
</ul>
</p>-->

<!-- Lecture 10 -->
<!--                <hr>
                <h2>Lecture 10: Pretraining Language Models (<a href="slides/lec10-pretraining-language-models.pdf">slides</a>)</h2>
                <p><i>introduction to language models (LMs), history of neural LMs, pretrained LMs, encoder-based, decoder-based and encoder-decoder based pretraining</i></p>
                <p>Please study the following material in preparation for the class:</p>
                <h3>Required Reading:</h3>
                <ul class="default">
                <li><a href="https://www.aclweb.org/anthology/N18-1202/">Deep Contextualized Word Representations</a>, Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, NAACL 2018.</li>
                <li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a>, Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, OpenAI Report, 2018.</li>
                <li><a href="https://www.aclweb.org/anthology/N19-1423/">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL 2019.</li>
                <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>, Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, ArXiv preprint ArXiv:1907.11692, 2019.</li>
                <li> <a href="https://openreview.net/forum?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a>, Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning, ICLR 2020.</li>
                <li> <a href="">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu, JMLR 21(140), 2020.</li>
                </ul>
                <p>
                <h3>Suggested Video Material:</h3>
                <ul class="default">
                  <li>Jacob Devlin's lecture on <a href="https://www.youtube.com/watch?v=knTc-NQSjKA">BERT and Other Pre-trained Language Models</a></li>
                </ul><br>
                </p>
                <p>
                <h3>Additional Resources:</h3>
                <ul class="default">
                  <li>[Blog post] <a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a>, Jay Alammar.</li>
                  <li>[Blog post] <a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html">Generalized Language Models</a>, Lilian Weng.</li>
                  <li><a href="https://www.aclweb.org/anthology/2020.tacl-1.54/">A Primer in BERTology: What we know about how BERT works</a>, Anna Rogers, Olga Kovaleva, Anna Rumshisky, TACL, Vol. 8, 2020.</li>
                  <li><a href="https://arxiv.org/abs/2205.05131">Unifying Language Learning Paradigms</a>, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler, arXiv preprint, 2025.</li>
                </ul>
                </p> 
-->
           				
<!-- Lecture 9 -->
<!--   				<hr>
  				<h2>Lecture 9: Graph Neural Networks (<a href="slides/lec9-graph_neural_networks.pdf">slides</a>)</h2>
  				<p><i>graph structured data, graph neural nets (GNNs), GNNs for ”classical” network problems</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
	  			<ul class="default">
		  			<li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a>, Thomas Kipf, Max Welling, ICLR 2017</li>
		  			<li><a href="https://arxiv.org/abs/1806.01261">Relational inductive biases, deep learning, and graph networks</a>, Peter W. Battaglia et al., arXiv Preprint arXiv:1806.01261, 2018</li>
		  		</ul>
	  			</p>
	  			
	  			<p>
	  			<h3>Suggested Video Material:</h3>
	  			<ul class="default">
		  			<li><a href="https://www.youtube.com/watch?v=sTGKOUzIpaQ">Principles and applications of relational inductive biases in deep learning</a>, Kelsey Allen, MIT CBMM Talks</li>
		 		</ul><br>
	  			
	  			<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://dl.acm.org/doi/10.1145/3503043">A Practical Tutorial on Graph Neural Networks</a>, Isaac Ronald Ward, Jack Joyner, Casey Lickfold, Yulan Guo, Mohammed Bennamoun, ACM Computing Surveys, Vol. 54, No: 10, September 2022.</li>
	  				<li><a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</a>, Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, Alexander B. Wiltschko, Distill, 2021</li>
	  				<li>[Blog post] <a href="http://tkipf.github.io/graph-convolutional-networks/">Graph Convolutional Networks</a>, Thomas Kipf</li>
		   					
  				</ul>
  				</p>
-->
  				
<!-- Lecture 8 -->
<!--   				<hr>
  				<h2>Lecture 8: Attention and Transformers (<a href="slides/lec8-attention-and-transformers.pdf">slides</a>)</h2>
  				<p><i>content-based attention, location-based attention, soft vs. hard attention, self-attention, attention for image captioning, transformer networks, vision transformers</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
	  			<ul class="default">
		  			<li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a>, Chris Olah and Shan Carter. Distill, 2016</li>	
		  			<li>[Blog post] <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>, Jay Alammar</li>	
		  			<li>[Blog post] <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">Transformers for Image Recognition at Scale</a>, Neil Houlsby and Dirk Weissenborn</li>
		  		</ul>
	  			</p>
	  			
	  			<p>
	  			<h3>Suggested Video Material:</h3>
	  			<ul class="default">
		  			<li><a href="https://www.youtube.com/watch?v=Q57rzaHHO0k">Attention and Memory in Deep Learning</a>, Alex Graves</li>
		  			<li><a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">Attention is all you need attentional neural network models</a>, Łukasz Kaiser</li>
		  			<li><a href="https://www.youtube.com/playlist?list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6">Vision Transformers explained</a>, AI Coffee Break with Letitia</li>
	  			</ul><br>
	  			
	  			<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>, D. Bahdanau, K. Cho, Y. Bengio, ICLR 2015</li>
	  				<li><a href="https://distill.pub/2017/ctc/">Sequence Modeling with CTC</a>, Awni Hannun, Distill, 2017</li>
		  			<li><a href="https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf">Recurrent Models of Visual Attention</a>, V. Mnih, N. Heess, A. Graves, K. Kavukcuoglu, NIPS 2014</li>
		  			<li><a href="http://proceedings.mlr.press/v37/gregor15.pdf">DRAW: a Recurrent Neural Network for Image Generation</a>, K. Gregor, I. Danihelka, A. Graves, DJ Rezende, D. Wierstra, ICML 2015</li>
		  			<li><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention Is All You Need</a>, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS 2017</li>
  					<li>[Blog post] <a href="http://kvfrans.com/what-is-draw-deep-recurrent-attentive-writer/">What is DRAW (Deep Recurrent Attentive Writer)?</a>, Kevin Frans</li>
  					<li>[Blog post] <a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html">The Transformer Family</a>, Lilian Weng</li>
  					
  				</ul>
  				</p>
-->

<!-- Lecture 7 -->
<!--
           <h2>Lecture 7: Recurrent Neural Networks (<a href="slides/lec7-recurrent-neural-nets.pdf">slides</a>)</h2>
  				<p><i>sequence modeling, recurrent neural networks (RNNs), RNN applications, vanilla RNN, training RNNs, long short-term memory (LSTM), LSTM variants, gated recurrent unit (GRU)</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/rnn.html">Chapter #10</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  					<li>Section 5 of <a href="https://arxiv.org/abs/1308.0850">Generating Sequence with Recurrent Neural Networks</a>, A. Graves, ArXiV</li>
		  			
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Efstratios Gavves and Max Welling's <a href="http://webcolleges.uva.nl/Mediasite/Play/00584cefc05647a3a47113c749dccac21d">Lecture 8</a></li>
  					<li><a href="https://www.youtube.com/watch?v=Keqep_PKrY8">Recurrent Neural Networks and Language Models</a>, Richard Socher</li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>[Blog post] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>, Chris Olah.</li>
  					<li>[Blog post] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>, Andrej Karpathy.</li>
  					<li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf">Learning Long-Term Dependencies with Gradient Descest is Difficult</a>, Yoshua Bengio, Patrice Simard, and Paolo Frasconi.</li>
  					<li><a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a>, Sepp Hochreiter and Jürgen Schmidhuber.</li>
  				</ul>
  				</p>
 -->
  				
 <!-- Lecture 6 -->
 <!--  				<hr>
  				<h2>Lecture 6: Understanding and Visualizing Convolutional Neural Networks (<a href="slides/lec6-understanding_convnets.pdf">slides</a>)</h2>
  				<p><i>transfer learning, interpretability, visualizing neuron activations, visualizing class activations, pre-images, adversarial examples, adversarial training</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li>Matthew D Zeiler and Rob Fergus, <a href="https://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a>, ECCV 2014.</li>
  					<li>Christian Szegedy et al. <a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>, arXiv preprint arXiv:1312.6199v4
</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Andrej Karpathy's Stanford CS231n <a href="https://www.youtube.com/watch?v=ta5fdaqDT3M">Lecture 9</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>[Blog post] <a href="http://yosinski.com/deepvis">Understanding Neural Networks Through Deep Visualization</a>, Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.</li>
  					<li>[Blog post] <a href="https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability</a>, Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye and Alexander Mordvintsev.</li>  	
  					<li>[Blog post] <a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a>, Chris Olah, Alexander Mordvintsev and Ludwin Schubert.</li>				
  					<li>[Blog post] <a href="https://distill.pub/2020/circuits/early-vision/">An Overview of Early Vision in InceptionV1</a>, Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter.</li>
  					<li>[Blog post] <a href="https://microscope.openai.com/models">OpenAI Microscope</a>.</li>
  					<li>[Blog post] <a href="http://karpathy.github.io/2015/03/30/breaking-convnets/">Breaking Linear Classifiers on ImageNet</a>, Andrej Karpathy.</li>
  					<li>[Blog post] <a href="https://openai.com/blog/adversarial-example-research/">Attacking machine learning with adversarial examples</a>, OpenAI.</li>
  				</ul>
  				</p>
-->
  				
<!-- Lecture 5 -->
<!--   				<hr>
  				<h2>Lecture 5: Convolutional Neural Networks (<a href="slides/lec5-convnets.pdf">slides</a>)</h2>
  				<p><i>convolution layer, pooling layer, cnn architectures, design guidelines, semantic segmentation networks, addressing other tasks</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/convnets.html">Chapter #9</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> textbook.</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Andrej Karpathy's Stanford CS231n <a href="https://www.youtube.com/watch?v=LxfUGhug-iQ">Lecture 7</a></li>
  					<li>Justin Johnson's Stanford CS231n <a href="https://www.youtube.com/watch?v=GxZrEKZfW2o">Lecture 8</a></li>
  					<li>Kaiming He's tutorial on <a href="https://www.youtube.com/watch?v=C6tLw-rPQ2o">Deep Residual Networks</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>Andrej Karpathy's CS231n notes on <a href="http://cs231n.github.io/convolutional-networks/">Convolutional Networks</a>.</li>
  					<li>Hiroshi Kuwajima’s Memo on <a href="https://www.slideshare.net/kuwajima/cnnbp">Backpropagation in Convolutional Neural Networks</a>.</li>
  					<li><a href="https://arxiv.org/pdf/1603.07285.pdf">A guide to convolution arithmetic for deep learning</a>, Vincent Dumoulin and Francesco Visin.</li>
  					<li><a href="https://www.mitpressjournals.org/doi/pdf/10.1162/neco_a_00990">Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review</a>, Waseem Rawat and Zenghui Wang.</li>
  					<li>[Blog post] <a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">Understanding Convolutions</a>, Christopher Olah.</li>
  					<li>[Blog post] <a href="https://distill.pub/2016/deconv-checkerboard/">Deconvolution and Checkerboard Artifacts</a>, Augustus Odena, Vincent Dumoulin, Chris Olah.</li>
  					<li>[Blog post] <a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9">Deep Learning for Object Detection: A Comprehensive Review</a>, Joyce Xu.</li>
  					<li>[Blog post] <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4">A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN</a>, Dhruv Parthasarathy</li>
  				</ul>
  				</p>
-->
  				
				<!-- Lecture 4 -->
    				<hr>
  				<h2>Lecture 4: Training Deep Neural Networks (<a href="slides/lec4-training-deep-nets.pdf">slides</a>)</h2>
  				<p><i>data preprocessing, weight initialization, normalization, regularization, model ensembles, dropout, optimization methods</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/regularization.html">Chapter #7</a> and <a href="http://www.deeplearningbook.org/contents/optimization.html">Chapter #8</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Efstratios Gavves' <a href="https://webcolleges.uva.nl/Mediasite/Play/947ccbc9b11940c0ad5ab39ebb154c461d">Lecture 3</a>.</li>
  					<li>Andrej Karpathy's video lecture titled <a href="https://www.youtube.com/watch?v=P6sfmUTpUmc">Building makemore Part 3: Activations & Gradients, BatchNorm</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="http://davidrosenberg.github.io/ml2015/refs/bottou-sgd-tricks-2012.pdf">Stochastic Gradient Descent Tricks</a>, Leon Bottou.</li>
  					<li>Section 3 of <a href="https://arxiv.org/pdf/1206.5533v1.pdf">Practical Recommendations for Gradient-Based Training of Deep Architectures</a>, Yoshua Bengio.</li>
  					<li><a href="http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf">Troubleshooting Deep Neural Networks: A Field Guide to Fixing Your Model</a>, Josh Tobin.</li>
  					<li>[Blog post] <a href="https://www.deeplearning.ai/ai-notes/initialization/">Initializing neural networks</a>, Katanforoosh & Kunin, deeplearning.ai.</li>
  					<li>[Blog post] <a href="https://www.deeplearning.ai/ai-notes/optimization/">Parameter optimization in neural networks</a>, Katanforoosh et al., deeplearning.ai.
  					<li>[Blog post] <a href="https://nmarkou.blogspot.com/2017/02/the-black-magic-of-deep-learning-tips.html">The Black Magic of Deep Learning - Tips and Tricks for the practitioner</a>, Nikolas Markou.</li>
  					<li>[Blog post] <a href="https://www.ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a>, Sebastian Ruder.</li>
  					<li>[Blog post] <a href="http://distill.pub/2017/momentum/">Why Momentum Really Works</a>, Gabriel Goh</li>  	
  				</ul>
  				</p>

  				
<!-- Lecture 3 -->
  				<hr>
  				<h2>Lecture 3: Multi-layer Perceptrons (<a href="slides/lec3-mlp.pdf">slides</a>)</h2>
  				<p><i>feed-forward neural networks, activation functions, chain rule, backpropagation, computational graph, automatic differentiation, distributed word representations</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="https://www.deeplearningbook.org/contents/mlp.html">Chapter 6</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  					<li>Yoav Goldberg's <a href="https://arxiv.org/pdf/1510.00726.pdf">A Primer on Neural Network Models for Natural Language Processing</a>, 3 to 6</li>
  					<li>Mathieu Blondel's presentation on <a href="https://mblondel.org/teaching/autodiff-2020.pdf">Automatic differentiation</a></li>
  					<li>[Blog post] <a href="https://jonaslalin.com/2021/10/12/forward-vs-reverse-accumulation-mode/">How Backpropagation Is Able To Reduce the Time Spent on Computing Gradients</a></li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li><a href="http://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Hugo Larochelle’s video lectures</a>, 1.1 to 1.6, 2.1 to 2.7</li>
  					<li>Andrej Karpathy's video lecture titled <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>Hinton's <a href="https://www.youtube.com/watch?v=2fRnHVVLf1Y&list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0">Coursera class</a> on Neural Networks, Lecture 1 to 3.</li>
  					<li>[Blog post] <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">Neural Networks, Manifolds, and Topology</a>, Christopher Olah.</li>
  					<li>[Blog post] <a href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs: Backpropagation</a>, Christopher Olah.</li>
  					<li><a href="https://web.stanford.edu/~jurafsky/slp3/16.pdf">Chapter 16</a> of Jurafsky and Martin's <a href="https://web.stanford.edu/~jurafsky/slp3">Speech and Language Processing book</a> (3rd Edition draft)</li>
  				</ul>
  				</p>
        

  				
<!-- Lecture 2 -->
   				<hr>
  				<h2>Lecture 2: Machine Learning Overview (<a href="slides/lec2-ml-overview.pdf">slides</a>)</h2>
  				<p><i>types of machine learning problems, linear models, loss functions, linear regression, gradient descent, overfitting and generalization, regularization, cross-validation, bias-variance tradeoff, maximum likelihood estimation</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/ml.html">Chapter 5</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li><a href="http://videolectures.net/deeplearning2016_precup_machine_learning/">Machine Learning</a>, Doina Precup (Deep Learning Summer School, Montreal 2016)</li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li><a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A few useful things to know about machine learning</a>, P. Domingos. Communications of the ACM, 55 (10), 78-87, 2012.</li>
  					<li><a href="https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/">The uneasy relationship between deep learning and (classical) statistics</a>, Boaz Barak, June 2022.</li>
  				</ul>
  				</p> 

  				  				  	
<!-- Lecture 1 -->  						
  				<hr>
  				<h2>Lecture 1: Introduction to Deep Learning (<a href="slides/lec1-introduction.pdf">slides</a>)</h2>
  				<p><i>course information, what is deep learning, a brief history of deep learning, compositionality, end-to-end learning, distributed representations</i></p>
  				
  				<p>Please study the following material in preparation for the class:</p>
  				<p>
  				<h3>Required Reading:</h3>
	  			<ul class="default">
		  			<li><a href="http://www.deeplearningbook.org/contents/intro.html">Chapter 1</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
	  				<li>[Blog post] <a href="https://pmirla.github.io/2016/08/16/AI-Winter.html">AI Winter. How Canadians contributed to end it?</a>, Pavan Mirla.</li>
	  				<li><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1056774">The Bandwagon</a>, Claude E. Shannon. IRE Transactions on Information Theory, Vol. 2, Issue 3, 1956</li>	
	  				<li><a href="readings/Marr-Chapter_01-The_Philosophy_and_the_Approach.pdf">Chapter 1: The Philosophy and the Approach</a> of David Marr's Vision, 1982.</li> 
	  			</ul>
  				</p>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://www.pnas.org/content/early/2020/01/23/1907373117">The unreasonable effectiveness of deep learning in artificial intelligence</a>, Terrence J. Sejnowski, PNAS, 2020.</li>
  					<li><a href="http://www.readcube.com/articles/10.1038%2Fnature14539">Deep Learning</a>, Yann LeCun, Yoshio Bengio, Geoffrey Hinton. Nature, Vol. 521, 2015.</li>
  					<li><a href="https://arxiv.org/abs/1404.7828">Deep Learning in Neural Networks: An Overview</a>, Juergen Schmidhuber. Neural Networks, Vol. 61, pp. 85–117, 2015.</li>
  					<li><a href="https://arxiv.org/pdf/1702.07800.pdf">On the Origin of Deep Learning</a>, Haohan Wang and Bhiksha Raj, arXiv preprint arXiv:1702.07800v4, 2017</li>
	  			</ul>
  				<hr>
  				</div>
  

	<!-- Footer -->
		<div id="footer">
			<!-- Copyright -->
				<div id="copyright">
					design: <a href="http://templated.co">templated.co</a>
				</div>			
		</div>

	</body>
</html>
