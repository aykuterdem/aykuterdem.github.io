<!DOCTYPE HTML>
<!--
	Solarize by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>COMP541: Deep Learning</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script  src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<script defer src="https://use.fontawesome.com/releases/v5.0.13/js/all.js"></script>
		<link href="css/fontawesome.css" rel="stylesheet">
		<link href="css/brands.css" rel="stylesheet">
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
	</head>
	<body class="homepage">

		<!-- Header Wrapper -->
			<div class="wrapper style11">
			
			<!-- Header -->
				<div id="header" style="color: #bbb">
					<div class="container">
						<!-- Logo -->
							<h1><a href="#" id="logo">COMP541</a></h1>

						<nav id="nav">
								<ul>
									<li class="active"><a href="index.html#div_courseinfo">About</a></li>
									<li><a href="index.html#div_schedule">Schedule</a></li>
									<li><a href="project.html">Project</a></li>
									<li><a href="assignments.html">Assignments</a></li>
									<li><a href="tutorials.html">Tutorials</a></li>
									<li><a href="presentations.html">Presentations</a></li>
									<li>
										<a href="http://ku.blackboard.com"><i class="fas fa-chalkboard"></i></a><!-- &middot;
										<a href="https://edstem.org/us/join/SdAA2p"<b>ed</b></a>-->
									</li>								
								</ul>
							</nav>
	
					</div>
				</div>
				
			<!-- Banner -->
				<div id="banner" style="color: black">
					<section class="container">
						<h2>COMP541: Deep Learning</h2>
						<span>Fall 2023</span>
					</section>
				</div>
			</div>

			<!-- Course Information -->
			<div class="wrapper style2">
			<section class="container">
            <h1 class="content-subhead">Detailed Syllabus and Lectures</h1>
<!-- Lecture 14 -->
<!--                <hr>
                <h2>Lecture 14: Massive Models and Scaling Laws (<a href="slides/lec14-massive-models-scaling-laws.pdf">slides</a>)</h2>
                <p><i>scaling laws, massive text models, applications of massive models</i></p>
                <p>Please study the following material in preparation for the class:</p>
                <h3>Required Reading:</h3>
                <ul class="default">
                    <li>[Blog post] <a href="https://openai.com/blog/ai-and-compute/">AI and Compute</a>, OpenAI.</li>
                    <li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a>, Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, 2020.</li>
                    <li><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a>, Jordan Hoffmann et al., 2022.</li>
                </ul>
                </p>
                <h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Mohit Iyyer's lecture on <a href="https://youtu.be/eQcOPH9QpcI">Scaling Laws for Large LMs</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
                <ul class="default">
                    <li>[Blog post] <a href="https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours">What Language Model to Train if You Have One Million GPU Hours?</a>, Julien Launay.</li>
                </ul>
-->
                
<!-- Lecture 13 -->
<!--   				<hr>
  				<h2>Lecture 13: Self-supervised Learning (<a href="slides/lec13-self-supervised-learning.pdf">slides</a>)</h2>
  				<p><i>what is self-supervised learning, self-supervised learning in NLP, self-supervised learning in vision, multimodal self-supervised learning</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
	  				<li>[Blog post] <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence">Self-supervised learning: The dark matter of intelligence</a>, Yann LeCun and Ishan Misra.</li>
  					<li>[Blog post] <a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html">Self-Supervised Representation Learning</a>, Lilian Weng.</li> 
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Yann LeCun's PAISS 2019 talk on <a href="https://www.youtube.com/watch?v=SaJL4SLfrcY">Self-Supervised Learning</a></li>
  					<li>Ishan Misra's lecture on <a href="https://www.youtube.com/watch?v=0KeR6i1_56g">Self-supervised learning (SSL) in computer vision</a></li>
  					<li>Andrei Bursuc, Spyros Gidaris, Aäron van den Oord, Spyros Gidaris, Andrei Bursuc, Mathilde Caron, Jean-Baptiste Alayrac and Adrià Recasen's tutorial titled <a href="https://www.youtube.com/watch?v=MdD4UMshl1Q">Leave Those Nets Alone: Advances in Self-Supervised Learning</a> at CVPR 2021.</li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, NIPS 2013.</li>
  					<li><a href="https://www.aclweb.org/anthology/N19-1423/">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL 2019.</li>
  					<li><a href="https://arxiv.org/abs/1604.07379">Context Encoders: Feature Learning by Inpainting</a>, Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell,
Alexei A. Efros, CVPR 2016.</li>
  					<li><a href="https://arxiv.org/abs/1505.05192">Unsupervised Visual Representation Learning by Context Prediction</a>, Carl Doersch, Abhinav Gupta, Alexei A. Efros</li>
  					<li><a href="https://arxiv.org/abs/1803.07728">Unsupervised Representation Learning by Predicting Image Rotations</a>, Spyros Gidaris, Praveer Singh, Nikos Komodakis, ICLR 2018.</li>
  					<li><a href="https://arxiv.org/abs/1807.03748">Representation Learning with Contrastive Predictive Learning</a>, Aaron van den Oord, Yazhe Li, Oriol Vinyals, ICLR 2018.</li>				
  					<li><a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a>, Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv preprint arXiv:2002.05709, 2020.</li>
  					<li><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf">Revisiting Self-Supervised Visual Representation Learning
</a>, Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer, CVPR 2019.</li>
  				</ul>
  				</p>
-->
  				
<!-- Lecture 12 -->
<!--   				<hr>
  				<h2>Lecture 12: Variational Autoencoders, Denoising Diffusion Models (<a href="slides/lec12-vaes-diffusion-models.pdf">slides</a>)</h2>
  				<p><i>variational autoencoders (VAEs), vector quantized variational autoencoders (VQ-VAEs), denoising diffusion models</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a>, Carl Doersch.</li> 
  					<li><a href="https://arxiv.org/abs/1906.02691">An Introduction to Variational Autoencoders</a>, Diederik P. Kingma, Max Welling.</li>
  					<li><a href="https://arxiv.org/abs/2209.00796">Diffusion Models: A Comprehensive Survey of Methods and Applications</a>, Ling Yang et al.</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li><a href="https://www.youtube.com/watch?v=DWVlEw0D3gA">Generative networks (variational autoencoders and GANs)</a>, Pascal Poupart</li>
  					<li><a href="https://www.youtube.com/watch?v=cS6JQpEY9cs">Denoising Diffusion-based Generative Modeling: Foundations and Applications</a>, Karsten Kreis, Ruiqi Gao and Arash Vahdat</li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>[Blog post] <a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">Intuitively Understanding Variational Autoencoders</a>, Irhum Shafkat.</li>
  					<li>[Blog post] <a href="https://blog.evjang.com/2016/08/variational-bayes.html">A Beginner's Guide to Variational Methods: Mean-Field Approximation</a>, Eric Jang.</li>
  					<li>[Blog post] <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Tutorial - What is a variational autoencoder?</a>, Jaan Altosaar</li>  
  					<li><a href="">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</a>, Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner, ICLR 2017.</li>			
  					<li><a href="https://arxiv.org/pdf/1811.12359">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</a>, Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem.</li>
  					<li><a href="https://arxiv.org/pdf/1906.00446">Generating Diverse High-Fidelity Images with VQ-VAE-2</a>, Ali Razavi, Aaron van den Oord, Oriol Vinyals.</li>	
  					<li>[Blog post] <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a>, Lilian Weng.</li>
  					<li><a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>, Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, CVPR 2022.</li>	
  				</ul>
  				</p>
-->


<!-- Lecture 11 -->
<!--   				<hr>
  				<h2>Lecture 11: Generative Adversarial Networks, Flow-Based Models (<a href="slides/lec11-gans-flow-based-models.pdf">slides</a>)</h2>
  				<p><i>generative adversarial networks (GANs), conditional GANs, applications of GANs, normalizing flows</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="https://arxiv.org/abs/1701.00160">NIPS 2016 Tutorial: Generative Adversarial Networks</a>, Ian Goodfellow</li> 
  					<li><a href="https://arxiv.org/abs/1710.07035">Generative Adversarial Networks: An Overview</a>, Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A Bharath</li>
  					<li><a href="https://github.com/soumith/ganhacks">How to Train a GAN? Tips and tricks to make GANs work</a>, Soumith Chintala, Emily Denton, Martin Arjovsky, Michael Mathieu</li>
  					<li>[Blog post] <a href="https://blog.evjang.com/2018/01/nf1.html">Normalizing Flows Tutorial, Part 1: Distributions and Determinants</a>, Eric Jang</li>
  					<li>[Blog post] <a href="https://blog.evjang.com/2018/01/nf2.html">Normalizing Flows Tutorial, Part 2: Modern Normalizing Flows</a>, Eric Jang</li>
  					<li>[Blog post] <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">Flow-based Deep Generative Models</a>, Lilian Weng</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li><a href="https://www.youtube.com/watch?v=AJVyzd0rqdc">NIPS 2016 Tutorial: Generative Adversarial Networks</a>, Ian Goodfellow</li>
  					<li><a href="https://www.youtube.com/watch?v=P4Ta-TZPVi0">A primer on normalizing flows</a>, Laurent Dinh</li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>[Blog post] <a href="https://github.com/soumith/ganhacks">How to Train a GAN? Tips and tricks to make GANs work</a>, Soumith Chintala, Emily Denton, Martin Arjovsky and Michael Mathieu.</li>
  					<li>[Blog post] <a href="https://github.com/hindupuravinash/the-gan-zoo">The GAN Zoo</a>, Avinash Hindupur</li>
  					<li>[Blog post] <a href="https://reiinakano.github.io/gan-playground/">GAN Playground</a>, Reiichiro Nakano</li>  
  					<li>[Blog post] <a href="https://github.com/khanrc/tf.gans-comparison">GANs comparison without cherry-picking</a>, Junbum Cha</li>
  					<li>[Twitter thread] <a href="https://twitter.com/goodfellow_ian/status/978339478560415744?lang=en">Thread on how to review papers about generic improvements to GANs</a>, Ian Goodfellow</li>
  					<li><a href="https://arxiv.org/pdf/1908.09257.pdf">Normalizing Flows: An Introduction and Review of Current Methods</a>, Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker, arXiv preprint, arXiv:1908.09257, 2020.</li>
  					<li><a href="https://arxiv.org/pdf/1912.02762.pdf">Normalizing Flows for Probabilistic Modeling and Inference</a>, George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, Balaji Lakshminarayanan, arXiv preprint, arXiv:1912.02762, 2019</li>
</li>
  					<li>[Blog post] <a href="https://openai.com/blog/glow/">Glow: Better Reversible Generative Models</a>, OpenAI</li>
  					<li><a href="https://openreview.net/forum?id=HkpbnH9lx">Density estimation using Real NVP</a>, Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio, ICLR 2017.</li>
  				</ul>
  				</p>
-->

<!-- Lecture 10 -->
   				<hr>
          <h2>Lecture 10: Autoregressive Models (<a href="slides/lec10-autoencoders-autoregressive-models.pdf">slides</a>)</h2>
          <p><i>unsupervised representation learning, sparse coding, autoencoders, autoregressive models</i></p>
          <p>Please study the following material in preparation for the class:</p>
          <h3>Required Reading:</h3>
          <ul class="default">
            <li><a href="http://www.deeplearningbook.org/contents/linear_factors.html">Chapter #13</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
            <li><a href="http://www.deeplearningbook.org/contents/autoencoders.html">Chapter #14</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
          </ul>
          <p>
          <h3>Suggested Video Material:</h3>
          <ul class="default"> 				
            <li><a href="https://www.youtube.com/watch?v=rK6bchqeaN8">Foundations of Unsupervised Deep Learning</a>, Ruslan Salakhutdinov</li>
            <li><a href="https://www.youtube.com/watch?v=R8fx2b8Asg0">Autoregressive Generative Models with Deep Learning</a>, Hugo Larochelle</li>
          </ul><br>
          </p>
          <h3>Additional Resources:</h3>
          <ul class="default">
            <li><a href="https://arxiv.org/pdf/1601.06759.pdf">Pixel Recurrent Neural Networks</a>, Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglum ICML2016.</li>
            <li><a href="https://arxiv.org/abs/1606.05328">Conditional Image Generation with PixelCNN Decoders</a>, Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu, NIPS2016.</li>
            <li><a href="http://ufldl.stanford.edu/tutorial/unsupervised">Unsupervised Feature Learning and Deep Learning</a>, Andrew Ng.</li>
            <li>[Blog post] <a href="https://blog.openai.com/unsupervised-sentiment-neuron/">Unsupervised Sentiment Neuron</a>, Alec Radford, Ilya Sutskever, Rafal Jozefowicz, Jack Clark and Greg.</li>
          </ul>
          </p>
 				
<!-- Lecture 9 -->
   				<hr>
  				<h2>Lecture 9: Graph Neural Networks (<a href="slides/lec9-graph_neural_networks.pdf">slides</a>)</h2>
  				<p><i>graph structured data, graph neural nets (GNNs), GNNs for ”classical” network problems</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
	  			<ul class="default">
		  			<li><a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a>, Thomas Kipf, Max Welling, ICLR 2017</li>
		  			<li><a href="https://arxiv.org/abs/1806.01261">Relational inductive biases, deep learning, and graph networks</a>, Peter W. Battaglia et al., arXiv Preprint arXiv:1806.01261, 2018</li>
		  		</ul>
	  			</p>
	  			
	  			<p>
	  			<h3>Suggested Video Material:</h3>
	  			<ul class="default">
		  			<li><a href="https://www.youtube.com/watch?v=sTGKOUzIpaQ">Principles and applications of relational inductive biases in deep learning</a>, Kelsey Allen, MIT CBMM Talks</li>
		 		</ul><br>
	  			
	  			<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://dl.acm.org/doi/10.1145/3503043">A Practical Tutorial on Graph Neural Networks</a>, Isaac Ronald Ward, Jack Joyner, Casey Lickfold, Yulan Guo, Mohammed Bennamoun, ACM Computing Surveys, Vol. 54, No: 10, September 2022.</li>
	  				<li><a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</a>, Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, Alexander B. Wiltschko, Distill, 2021</li>
	  				<li>[Blog post] <a href="http://tkipf.github.io/graph-convolutional-networks/">Graph Convolutional Networks</a>, Thomas Kipf</li>
		   					
  				</ul>
  				</p>

  				
<!-- Lecture 8 -->
   				<hr>
  				<h2>Lecture 8: Attention and Transformers (<a href="slides/lec8-attention-and-transformers.pdf">slides</a>)</h2>
  				<p><i>content-based attention, location-based attention, soft vs. hard attention, self-attention, attention for image captioning, transformer networks, vision transformers</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
	  			<ul class="default">
		  			<li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a>, Chris Olah and Shan Carter. Distill, 2016</li>	
		  			<li>[Blog post] <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>, Jay Alammar</li>	
		  			<li>[Blog post] <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">Transformers for Image Recognition at Scale</a>, Neil Houlsby and Dirk Weissenborn</li>
		  		</ul>
	  			</p>
	  			
	  			<p>
	  			<h3>Suggested Video Material:</h3>
	  			<ul class="default">
		  			<li><a href="https://www.youtube.com/watch?v=Q57rzaHHO0k">Attention and Memory in Deep Learning</a>, Alex Graves</li>
		  			<li><a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">Attention is all you need attentional neural network models</a>, Łukasz Kaiser</li>
		  			<li><a href="https://www.youtube.com/playlist?list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6">Vision Transformers explained</a>, AI Coffee Break with Letitia</li>
	  			</ul><br>
	  			
	  			<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>, D. Bahdanau, K. Cho, Y. Bengio, ICLR 2015</li>
	  				<li><a href="https://distill.pub/2017/ctc/">Sequence Modeling with CTC</a>, Awni Hannun, Distill, 2017</li>
		  			<li><a href="https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf">Recurrent Models of Visual Attention</a>, V. Mnih, N. Heess, A. Graves, K. Kavukcuoglu, NIPS 2014</li>
		  			<li><a href="http://proceedings.mlr.press/v37/gregor15.pdf">DRAW: a Recurrent Neural Network for Image Generation</a>, K. Gregor, I. Danihelka, A. Graves, DJ Rezende, D. Wierstra, ICML 2015</li>
		  			<li><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention Is All You Need</a>, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS 2017</li>
  					<li>[Blog post] <a href="http://kvfrans.com/what-is-draw-deep-recurrent-attentive-writer/">What is DRAW (Deep Recurrent Attentive Writer)?</a>, Kevin Frans</li>
  					<li>[Blog post] <a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html">The Transformer Family</a>, Lilian Weng</li>
  					
  				</ul>
  				</p>


<!-- Lecture 7 -->
  				<hr>
  				<h2>Lecture 7: Recurrent Neural Networks (<a href="slides/lec7-recurrent-neural-nets.pdf">slides</a>)</h2>
  				<p><i>sequence modeling, recurrent neural networks (RNNs), RNN applications, vanilla RNN, training RNNs, long short-term memory (LSTM), LSTM variants, gated recurrent unit (GRU)</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/rnn.html">Chapter #10</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  					<li>Section 5 of <a href="https://arxiv.org/abs/1308.0850">Generating Sequence with Recurrent Neural Networks</a>, A. Graves, ArXiV</li>
		  			
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Efstratios Gavves and Max Welling's <a href="http://webcolleges.uva.nl/Mediasite/Play/00584cefc05647a3a47113c749dccac21d">Lecture 8</a></li>
  					<li><a href="https://www.youtube.com/watch?v=Keqep_PKrY8">Recurrent Neural Networks and Language Models</a>, Richard Socher</li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>[Blog post] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>, Chris Olah.</li>
  					<li>[Blog post] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>, Andrej Karpathy.</li>
  					<li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf">Learning Long-Term Dependencies with Gradient Descest is Difficult</a>, Yoshua Bengio, Patrice Simard, and Paolo Frasconi.</li>
  					<li><a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a>, Sepp Hochreiter and Jürgen Schmidhuber.</li>
  				</ul>
  				</p>
  				
 <!-- Lecture 6 -->
   				<hr>
  				<h2>Lecture 6: Understanding and Visualizing Convolutional Neural Networks (<a href="slides/lec6-understanding_convnets.pdf">slides</a>)</h2>
  				<p><i>transfer learning, interpretability, visualizing neuron activations, visualizing class activations, pre-images, adversarial examples, adversarial training</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li>Matthew D Zeiler and Rob Fergus, <a href="https://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a>, ECCV 2014.</li>
  					<li>Christian Szegedy et al. <a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>, arXiv preprint arXiv:1312.6199v4
</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Andrej Karpathy's Stanford CS231n <a href="https://www.youtube.com/watch?v=ta5fdaqDT3M">Lecture 9</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>[Blog post] <a href="http://yosinski.com/deepvis">Understanding Neural Networks Through Deep Visualization</a>, Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson.</li>
  					<li>[Blog post] <a href="https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability</a>, Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye and Alexander Mordvintsev.</li>  	
  					<li>[Blog post] <a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a>, Chris Olah, Alexander Mordvintsev and Ludwin Schubert.</li>				
  					<li>[Blog post] <a href="https://distill.pub/2020/circuits/early-vision/">An Overview of Early Vision in InceptionV1</a>, Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter.</li>
  					<li>[Blog post] <a href="https://microscope.openai.com/models">OpenAI Microscope</a>.</li>
  					<li>[Blog post] <a href="http://karpathy.github.io/2015/03/30/breaking-convnets/">Breaking Linear Classifiers on ImageNet</a>, Andrej Karpathy.</li>
  					<li>[Blog post] <a href="https://openai.com/blog/adversarial-example-research/">Attacking machine learning with adversarial examples</a>, OpenAI.</li>
  				</ul>
  				</p>

  				
<!-- Lecture 5 -->
<!--   				<hr>
  				<h2>Lecture 5: Convolutional Neural Networks (<a href="slides/lec5-convnets.pdf">slides</a>)</h2>
  				<p><i>convolution layer, pooling layer, cnn architectures, design guidelines, semantic segmentation networks, addressing other tasks</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/convnets.html">Chapter #9</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> textbook.</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Andrej Karpathy's Stanford CS231n <a href="https://www.youtube.com/watch?v=LxfUGhug-iQ">Lecture 7</a></li>
  					<li>Justin Johnson's Stanford CS231n <a href="https://www.youtube.com/watch?v=GxZrEKZfW2o">Lecture 8</a></li>
  					<li>Kaiming He's tutorial on <a href="https://www.youtube.com/watch?v=C6tLw-rPQ2o">Deep Residual Networks</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>Andrej Karpathy's CS231n notes on <a href="http://cs231n.github.io/convolutional-networks/">Convolutional Networks</a>.</li>
  					<li>Hiroshi Kuwajima’s Memo on <a href="https://www.slideshare.net/kuwajima/cnnbp">Backpropagation in Convolutional Neural Networks</a>.</li>
  					<li><a href="https://arxiv.org/pdf/1603.07285.pdf">A guide to convolution arithmetic for deep learning</a>, Vincent Dumoulin and Francesco Visin.</li>
  					<li><a href="https://www.mitpressjournals.org/doi/pdf/10.1162/neco_a_00990">Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review</a>, Waseem Rawat and Zenghui Wang.</li>
  					<li>[Blog post] <a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">Understanding Convolutions</a>, Christopher Olah.</li>
  					<li>[Blog post] <a href="https://distill.pub/2016/deconv-checkerboard/">Deconvolution and Checkerboard Artifacts</a>, Augustus Odena, Vincent Dumoulin, Chris Olah.</li>
  					<li>[Blog post] <a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9">Deep Learning for Object Detection: A Comprehensive Review</a>, Joyce Xu.</li>
  					<li>[Blog post] <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4">A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN</a>, Dhruv Parthasarathy</li>
  				</ul>
  				</p>
-->
  				
 				<!-- Lecture 4 -->
   				<hr>
  				<h2>Lecture 4: Training Deep Neural Networks (<a href="slides/lec4-training-deep-nets.pdf">slides</a>)</h2>
  				<p><i>data preprocessing, weight initialization, normalization, regularization, model ensembles, dropout, optimization methods</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/regularization.html">Chapter #7</a> and <a href="http://www.deeplearningbook.org/contents/optimization.html">Chapter #8</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li>Efstratios Gavves' <a href="https://webcolleges.uva.nl/Mediasite/Play/947ccbc9b11940c0ad5ab39ebb154c461d">Lecture 3</a>.</li>
  					<li>Andrej Karpathy's video lecture titled <a href="https://www.youtube.com/watch?v=P6sfmUTpUmc">Building makemore Part 3: Activations & Gradients, BatchNorm</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="http://davidrosenberg.github.io/ml2015/refs/bottou-sgd-tricks-2012.pdf">Stochastic Gradient Descent Tricks</a>, Leon Bottou.</li>
  					<li>Section 3 of <a href="https://arxiv.org/pdf/1206.5533v1.pdf">Practical Recommendations for Gradient-Based Training of Deep Architectures</a>, Yoshua Bengio.</li>
  					<li><a href="http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf">Troubleshooting Deep Neural Networks: A Field Guide to Fixing Your Model</a>, Josh Tobin.</li>
  					<li>[Blog post] <a href="https://www.deeplearning.ai/ai-notes/initialization/">Initializing neural networks</a>, Katanforoosh & Kunin, deeplearning.ai.</li>
  					<li>[Blog post] <a href="https://www.deeplearning.ai/ai-notes/optimization/">Parameter optimization in neural networks</a>, Katanforoosh et al., deeplearning.ai.
  					<li>[Blog post] <a href="https://nmarkou.blogspot.com/2017/02/the-black-magic-of-deep-learning-tips.html">The Black Magic of Deep Learning - Tips and Tricks for the practitioner</a>, Nikolas Markou.</li>
  					<li>[Blog post] <a href="https://www.ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a>, Sebastian Ruder.</li>
  					<li>[Blog post] <a href="http://distill.pub/2017/momentum/">Why Momentum Really Works</a>, Gabriel Goh</li>  	
  				</ul>
  				</p>

  				
<!-- Lecture 3 -->
  				<hr>
  				<h2>Lecture 3: Multi-layer Perceptrons (<a href="slides/lec3-mlp.pdf">slides</a>)</h2>
  				<p><i>feed-forward neural networks, activation functions, chain rule, backpropagation, computational graph, automatic differentiation, distributed word representations</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="https://www.deeplearningbook.org/contents/mlp.html">Chapter 6</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  					<li>Yoav Goldberg's <a href="https://arxiv.org/pdf/1510.00726.pdf">A Primer on Neural Network Models for Natural Language Processing</a>, 3 to 6</li>
  					<li>Mathieu Blondel's presentation on <a href="https://mblondel.org/teaching/autodiff-2020.pdf">Automatic differentiation</a></li>
  					<li>[Blog post] <a href="https://jonaslalin.com/2021/10/12/forward-vs-reverse-accumulation-mode/">How Backpropagation Is Able To Reduce the Time Spent on Computing Gradients</a></li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li><a href="http://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Hugo Larochelle’s video lectures</a>, 1.1 to 1.6, 2.1 to 2.7</li>
  					<li>Andrej Karpathy's video lecture titled <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">The spelled-out intro to neural networks and backpropagation: building micrograd</a></li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li>Hinton's <a href="https://www.youtube.com/watch?v=2fRnHVVLf1Y&list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0">Coursera class</a> on Neural Networks, Lecture 1 to 3.</li>
  					<li>[Blog post] <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">Neural Networks, Manifolds, and Topology</a>, Christopher Olah.</li>
  					<li>[Blog post] <a href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs: Backpropagation</a>, Christopher Olah.</li>
  					<li><a href="https://web.stanford.edu/~jurafsky/slp3/16.pdf">Chapter 16</a> of Jurafsky and Martin's <a href="https://web.stanford.edu/~jurafsky/slp3">Speech and Language Processing book</a> (3rd Edition draft)</li>
  				</ul>
  				</p>

  				
<!-- Lecture 2 -->
   				<hr>
  				<h2>Lecture 2: Machine Learning Overview (<a href="slides/lec2-ml-overview.pdf">slides</a>)</h2>
  				<p><i>types of machine learning problems, linear models, loss functions, linear regression, gradient descent, overfitting and generalization, regularization, cross-validation, bias-variance tradeoff, maximum likelihood estimation</i></p>
  				<p>Please study the following material in preparation for the class:</p>
  				<h3>Required Reading:</h3>
  				<ul class="default">
  					<li><a href="http://www.deeplearningbook.org/contents/ml.html">Chapter 5</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
  				</ul>
  				<p>
  				<h3>Suggested Video Material:</h3>
  				<ul class="default">
  					<li><a href="http://videolectures.net/deeplearning2016_precup_machine_learning/">Machine Learning</a>, Doina Precup (Deep Learning Summer School, Montreal 2016)</li>
  				</ul><br>
  				</p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
  					<li><a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A few useful things to know about machine learning</a>, P. Domingos. Communications of the ACM, 55 (10), 78-87, 2012.</li>
  					<li><a href="https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/">The uneasy relationship between deep learning and (classical) statistics</a>, Boaz Barak, June 2022.</li>
  				</ul>
  				</p>
  				  				  	
<!-- Lecture 1 -->  						
  				<hr>
  				<h2>Lecture 1: Introduction to Deep Learning (<a href="slides/lec1-introduction.pdf">slides</a>)</h2>
  				<p><i>course information, what is deep learning, a brief history of deep learning, compositionality, end-to-end learning, distributed representations</i></p>
  				
  				<p>Please study the following material in preparation for the class:</p>
  				<p>
  				<h3>Required Reading:</h3>
	  			<ul class="default">
		  			<li><a href="http://www.deeplearningbook.org/contents/intro.html">Chapter 1</a> of the <a href="http://www.deeplearningbook.org/">Deep Learning</a> text book.</li>
	  				<li>[Blog post] <a href="https://pmirla.github.io/2016/08/16/AI-Winter.html">AI Winter. How Canadians contributed to end it?</a>, Pavan Mirla.</li>
	  				<li><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1056774">The Bandwagon</a>, Claude E. Shannon. IRE Transactions on Information Theory, Vol. 2, Issue 3, 1956</li>	
	  				<li><a href="readings/Marr-Chapter_01-The_Philosophy_and_the_Approach.pdf">Chapter 1: The Philosophy and the Approach</a> of David Marr's Vision, 1982.</li> 
	  			</ul>
  				</p>
  				
  				<p>
  				<h3>Additional Resources:</h3>
  				<ul class="default">
	  				<li><a href="https://www.pnas.org/content/early/2020/01/23/1907373117">The unreasonable effectiveness of deep learning in artificial intelligence</a>, Terrence J. Sejnowski, PNAS, 2020.</li>
  					<li><a href="http://www.readcube.com/articles/10.1038%2Fnature14539">Deep Learning</a>, Yann LeCun, Yoshio Bengio, Geoffrey Hinton. Nature, Vol. 521, 2015.</li>
  					<li><a href="https://arxiv.org/abs/1404.7828">Deep Learning in Neural Networks: An Overview</a>, Juergen Schmidhuber. Neural Networks, Vol. 61, pp. 85–117, 2015.</li>
  					<li><a href="https://arxiv.org/pdf/1702.07800.pdf">On the Origin of Deep Learning</a>, Haohan Wang and Bhiksha Raj, arXiv preprint arXiv:1702.07800v4, 2017</li>
	  			</ul>
  				<hr>
  				</div>
  

	<!-- Footer -->
		<div id="footer">
			<!-- Copyright -->
				<div id="copyright">
					design: <a href="http://templated.co">templated.co</a>
				</div>			
		</div>

	</body>
</html>
